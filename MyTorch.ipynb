{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MyTorch.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPlzya2xglzdPY5otnUwW/a",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mattbarrett98/mikit-learn/blob/main/MyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "xecyehlGtCfd"
      },
      "outputs": [],
      "source": [
        "# fast linear algebra, useful for every algorithm\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Base network"
      ],
      "metadata": {
        "id": "j1in42rtwVIg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BaseNetwork:\n",
        "    \"\"\"Base network from which all our networks will inherit since they\n",
        "    all have some shared functionality. They each have a classification accuracy\n",
        "    associated with them and need a method to calculate that accuracy. \n",
        "\n",
        "    We also have a magic method to allow us to compare our implementation to \n",
        "    PyTorch's. We consider the implementations to be equivalent if their\n",
        "    classification accuracies are within 0.5% of each other. \n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.accuracy = None\n",
        "\n",
        "    def softmax(self, x):\n",
        "        \"\"\"Softmax activation function for final layer of networks.\"\"\"\n",
        "        p = np.exp(x)\n",
        "        return p / np.sum(p, axis=0)\n",
        "\n",
        "    def relu(self, x):\n",
        "        \"\"\"Relu activation function. Note that max(x, 0) = (x + |x|)/2.\"\"\"\n",
        "        return (x + abs(x))/2     \n",
        "\n",
        "    def evaluate_predictions(self, predictions, true_classes):\n",
        "        n_correct_predictions = sum(predictions == true_classes)\n",
        "        n_predictions = predictions.shape[0]\n",
        "        self.accuracy = 100 * n_correct_predictions/n_predictions\n",
        "        return\n",
        "\n",
        "    def __eq__(self, pytorch_model):\n",
        "        diff = self.accuracy - pytorch_model.accuracy\n",
        "        diff = round(diff, 2)\n",
        "        if diff == 0:\n",
        "           return \"True, MyTorch's accuracy is the same as PyTorch's.\"\n",
        "        if diff > 0 and diff <= 0.5:\n",
        "           return f\"True, MyTorch's accuracy is just {diff}% higher than PyTorch's.\"\n",
        "        if diff > 0 and diff > 0.5:\n",
        "           return f\"False, MyTorch's accuracy is {diff}% higher than PyTorch's.\"   \n",
        "        \n",
        "        if diff < 0 and diff >= -0.5:\n",
        "           return f\"True, MyTorch's accuracy is just {-diff}% lower than PyTorch's.\"  \n",
        "        if diff < 0 and diff < -0.5:\n",
        "           return f\"False, MyTorch's accuracy is {-diff}% lower than PyTorch's.\""
      ],
      "metadata": {
        "id": "ueVWUpmTtdvw"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multilayer perceptron"
      ],
      "metadata": {
        "id": "S65-kNdZwZEz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyMLP(BaseNetwork):\n",
        "    \"\"\"Classifier based on a multilayer perceptron. We use 2 hidden layers, relu\n",
        "    activations for hidden layers and softmax activation for output layer. We \n",
        "    use the negative log likelihood as our loss function and use the Adam \n",
        "    optimisation algorithm to minimise it. \n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    epochs : int. The number of times we loop through the whole dataset during\n",
        "    training. \n",
        "\n",
        "    batch_size : int. The number of training samples we use to calculate the \n",
        "    loss for each stochastic optimisation step.\n",
        "\n",
        "    layer_sizes : list of ints. The number of neurons in each layer of the \n",
        "    network, including the input and output layer. \n",
        "\n",
        "    learning_rate : float. Controls step sizes in Adam. \n",
        "\n",
        "    beta_1 : float, must be in [0,1). Decay rate for the first moments in Adam.\n",
        "\n",
        "    beta_2 : float, in [0,1). Decay rate for second moments in Adam.   \n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        epochs,\n",
        "        batch_size,\n",
        "        layer_sizes,\n",
        "        learning_rate,\n",
        "        beta_1,\n",
        "        beta_2\n",
        "    ):\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.layer_sizes = layer_sizes\n",
        "        self.learning_rate = learning_rate\n",
        "        self.beta_1 = beta_1\n",
        "        self.beta_2 = beta_2\n",
        "        self.optimal_par = None\n",
        "\n",
        "    def _unpack(self, par):\n",
        "        \"\"\"Takes a flattened vector of parameters, par, and returns lists of \n",
        "        arrays of weights and biases for each network layer.  \n",
        "        \"\"\"\n",
        "        lower_idx = upper_idx = 0\n",
        "        weights = []\n",
        "        biases = []\n",
        "        for i in range(len(self.layer_sizes) - 1):\n",
        "            n_neurons = self.layer_sizes[i]\n",
        "            n_neurons_next = self.layer_sizes[i + 1]\n",
        "            upper_idx += n_neurons * n_neurons_next\n",
        "            w = par[lower_idx:upper_idx].reshape(n_neurons_next, n_neurons)\n",
        "            b = par[upper_idx:upper_idx + n_neurons_next]\n",
        "            weights.append(w)\n",
        "            biases.append(b)\n",
        "            upper_idx += n_neurons_next\n",
        "            lower_idx = upper_idx\n",
        "        return weights, biases\n",
        "\n",
        "    def _pack(self, *parameters):\n",
        "        \"\"\"Takes any number of parameter arrays, and returns all the parameters\n",
        "        as one flattened vector. \n",
        "        \"\"\"\n",
        "        packed_par = parameters[0].ravel()\n",
        "        for i in range(1, len(parameters)):\n",
        "            packed_par = np.concatenate((packed_par, parameters[i].ravel()))\n",
        "        return packed_par\n",
        "\n",
        "    def _mlp_grad(self, batch, truth, parameters):\n",
        "        \"\"\"This function first makes a forward pass through the network to find \n",
        "        the outputs. Using the outputs and backpropogation we make a backward \n",
        "        pass through the network to find all the gradients. \n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        batch : numpy array of shape (n_features, batch_size) containing batch\n",
        "        of training data.\n",
        "\n",
        "        truth : numpy array of shape (n_classes, batch_size) containing the one\n",
        "        hot encoded true classes of each batch observation. \n",
        "\n",
        "        parameters : vector containing all network parameter values.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        flattened vector containing gradients of the loss wrt all parameters. \n",
        "        \"\"\"\n",
        "        weights, biases = self._unpack(parameters)\n",
        "        z1 = np.matmul(weights[0], batch) + biases[0][:, np.newaxis]\n",
        "        a1 = self.relu(z1)\n",
        "        z2 = np.matmul(weights[1], a1) + biases[1][:, np.newaxis]\n",
        "        a2 = self.relu(z2)\n",
        "        # output of neural network\n",
        "        a3 = self.softmax(np.matmul(weights[2], a2) + biases[2][:, np.newaxis])\n",
        "        # calculate gradients using the chain rule with the negative log\n",
        "        # likelihood as our loss function\n",
        "        inter1 = a3 - truth\n",
        "        grad_b2 = np.sum(inter1, axis=1)\n",
        "        grad_w2 = np.matmul(inter1, a2.T)\n",
        "        inter2 = np.matmul(weights[2].T, inter1) * np.sign(a2)\n",
        "        grad_b1 = np.sum(inter2, axis=1)\n",
        "        grad_w1 = np.matmul(inter2, a1.T)\n",
        "        inter3 = np.matmul(weights[1].T, inter2) * np.sign(a1)\n",
        "        grad_b0 = np.sum(inter3, axis=1)\n",
        "        grad_w0 = np.matmul(inter3, batch.T)\n",
        "        return self._pack(grad_w0, grad_b0, grad_w1, grad_b1, grad_w2, grad_b2)\n",
        "\n",
        "    def _adam(self, batch, truth, par, learning_rate, beta_1, beta_2, m, v, t):\n",
        "        \"\"\"This function performs one optimisation step given by Adam. The\n",
        "        implementation and notation follows https://arxiv.org/abs/1412.6980 .\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        batch : numpy array of shape (n_features, batch_size) containing batch\n",
        "        of training data.\n",
        "\n",
        "        truth : numpy array of shape (n_classes, batch_size) containing the one\n",
        "        hot encoded true classes of each batch observation. \n",
        "\n",
        "        par : vector containing all network parameter values. \n",
        "\n",
        "        learning_rate : float. Controls the size of the optimisation step taken.\n",
        "\n",
        "        beta_1 : float, must be in [0,1). Decay rate for the first moment \n",
        "        estimate, 'm'.\n",
        "\n",
        "        beta_2 : float, in [0,1). Decay rate for second moment estimate, 'v'.\n",
        "\n",
        "        m : vector of first moment estimates.\n",
        "\n",
        "        v : vector of second moment estimates. \n",
        "\n",
        "        t : int. The current timestep. \n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        par, m, v, t : the updated values of the parameters, first moment\n",
        "        estimates, second moment estimates and timestep respectively.\n",
        "        \"\"\"\n",
        "        if t == 0:\n",
        "            m = v = np.zeros(par.shape[0])\n",
        "        t += 1\n",
        "        g = self._mlp_grad(batch, truth, par)\n",
        "        m = beta_1*m + (1 - beta_1)*g\n",
        "        v = beta_2*v + (1 - beta_2)*g*g\n",
        "        alpha = learning_rate * np.sqrt(1 - beta_2**t) / (1 - beta_1**t)\n",
        "        par -= alpha * m / (np.sqrt(v) + 1e-8)\n",
        "        return par, m, v, t\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"This function initialises the weights of the network and performs one \n",
        "        Adam optimisation step for each batch in each epoch. The trained weights \n",
        "        are stored in the attribute optimal_par.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        X : numpy array of shape (n_features, n_training_obs) containing the \n",
        "        training data.\n",
        "\n",
        "        y : numpy array of shape (n_classes, n_training_obs) containing the one \n",
        "        hot encoded true class of each training observation.\n",
        "        \"\"\"\n",
        "        # He initialisation since we are using relu activations\n",
        "        n0, n1, n2, n3 = self.layer_sizes\n",
        "        w0 = np.random.normal(0, np.sqrt(1 / n0), (n1, n0))\n",
        "        b0 = np.zeros(n1)\n",
        "        w1 = np.random.normal(0, np.sqrt(2 / n1), (n2, n1))\n",
        "        b1 = np.zeros(n2)\n",
        "        w2 = np.random.normal(0, np.sqrt(2 / n2), (n3, n2))\n",
        "        b2 = np.zeros(n3)\n",
        "        par = self._pack(w0, b0, w1, b1, w2, b2)\n",
        "        t = m = v = 0\n",
        "        for i in range(self.epochs):\n",
        "            # in each epoch perform gradient descent on each mini batch\n",
        "            for j in range(int(np.ceil(X.shape[1] / self.batch_size))):\n",
        "                batch = X[:, self.batch_size * j:self.batch_size * (j + 1)]\n",
        "                truth = y[:, self.batch_size * j:self.batch_size * (j + 1)]\n",
        "                par, m, v, t = self._adam(batch,\n",
        "                                          truth,\n",
        "                                          par,\n",
        "                                          self.learning_rate,\n",
        "                                          self.beta_1,\n",
        "                                          self.beta_2,\n",
        "                                          m,\n",
        "                                          v,\n",
        "                                          t\n",
        "                                          )\n",
        "        self.optimal_par = par\n",
        "        return self.optimal_par\n",
        "\n",
        "    def predict(self, test_x):\n",
        "        \"\"\"This returns the predicted classes of the test data given by the MLP.\n",
        "\n",
        "        Parameters \n",
        "        ----------\n",
        "        test_x : numpy array of shape (n_test_samples, n_features) containing\n",
        "        the test data.\n",
        "        \"\"\"\n",
        "        weights, biases = self._unpack(self.optimal_par)\n",
        "        z1 = np.matmul(weights[0], test_x.T) + biases[0][:, np.newaxis]\n",
        "        a1 = self.relu(z1)\n",
        "        z2 = np.matmul(weights[1], a1) + biases[1][:, np.newaxis]\n",
        "        a2 = self.relu(z2)\n",
        "        z3 = np.matmul(weights[2], a2) + biases[2][:, np.newaxis]\n",
        "        a3 = self.softmax(z3)\n",
        "        predictions = np.argmax(a3, axis=0)\n",
        "        return predictions"
      ],
      "metadata": {
        "id": "A9OXGA8vwbjq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convolutional neural network"
      ],
      "metadata": {
        "id": "hU9JS1lEkS3f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyCNN(BaseNetwork):\n",
        "    \"\"\"A classifier based on a convolutional neural network. We also make use of\n",
        "    a novel technique known as batch normalisation, details in:\n",
        "    https://arxiv.org/abs/1502.03167 .\n",
        "\n",
        "    CNN architecture\n",
        "    -----------------\n",
        "    - convolutional layer with 'n_filters_1' 2D filters of size ''filter_size'\n",
        "    followed by relu activation,\n",
        "    - 2x2 max pooling layer,\n",
        "    - batch normalisation layer,\n",
        "    - convolutional layer with 'n_filters_2' 2D filters of size ''filter_size'\n",
        "    followed by relu activation,\n",
        "    - 2x2 max pooling layer,\n",
        "    - batch normalisation layer,\n",
        "    - flatten,\n",
        "    - fully connected layer with 'n_dense' neurons and relu activation,\n",
        "    - batch normalisation layer,\n",
        "    - fully connected output layer with n_classes neurons and softmax activation\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    epochs : int. The number of times we loop through the whole dataset during\n",
        "    training. \n",
        "\n",
        "    batch_size : int. The number of training samples we use to calculate the \n",
        "    loss for each stochastic optimisation step.\n",
        "\n",
        "    filter_size : int. The size of the array of weights in each convolutional \n",
        "    layer will be filter_size x filter_size. \n",
        "\n",
        "    n_filters_1, n_filters_2 : int. Number of filters in the first and second\n",
        "    conv layers.\n",
        "\n",
        "    n_dense : int. Number of neurons in the fully connected layer. \n",
        "\n",
        "    learning_rate : float. Controls step sizes in stochastic gradient descent.\n",
        "\n",
        "    momentum : float, default=0.9. Must be in (0,1). Controls the momentum used\n",
        "    to compute the moving averages of means and variances in the batch \n",
        "    normalisation layers. \n",
        "\n",
        "    par : to store the parameters found from training. \n",
        "\n",
        "    mu, var : stores the finals approximations of the means and \n",
        "    variances for the batch norm layers.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        epochs,\n",
        "        batch_size,\n",
        "        filter_size,\n",
        "        n_filters_1,\n",
        "        n_filters_2,\n",
        "        n_dense,\n",
        "        learning_rate,\n",
        "        momentum=0.9\n",
        "    ):\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.filter_size = filter_size\n",
        "        self.n_filters_1 = n_filters_1\n",
        "        self.n_filters_2 = n_filters_2\n",
        "        self.n_dense = n_dense\n",
        "        self.learning_rate = learning_rate\n",
        "        self.momentum = momentum\n",
        "        self.par = None\n",
        "        self.mu = None\n",
        "        self.var = None\n",
        "\n",
        "    def _cnn_grad(self, batch, truth, f1, b1, gamma1, beta1, mu1_MA, var1_MA,\n",
        "                  f2, b2, gamma2, beta2, mu2_MA, var2_MA, w1, b3, gamma3, beta3,\n",
        "                  mu3_MA, var3_MA, w2, b4):\n",
        "        \"\"\"Computes the gradient of our negative log likelihood loss with \n",
        "        respect to all network parameters. Our implementation differs from the \n",
        "        frameworks TensorFlow and PyTorch, since they use automatic\n",
        "        differentiation whereas we have calculated the gradients directly.  \n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        batch : contains a batch of training samples.\n",
        "\n",
        "        truth : contains one hot encoded true classes of the training samples in\n",
        "        batch.\n",
        "\n",
        "        f1 : filter weights for first conv layer of shape \n",
        "        (n_filters_1, filter_size**2).\n",
        "\n",
        "        b1 : biases for first conv layer of shape (n_filters_1,).\n",
        "\n",
        "        gamma1 : scale factors for the first batch norm layer, following the\n",
        "        notation from the original paper. Shape (n_filters_1,).\n",
        "\n",
        "        beta1 : shift factors for first batch norm layer of shape (n_filters_1,)\n",
        "\n",
        "        mu1_MA, var1_MA : moving averages of the mean and variance for first \n",
        "        batch norm layer.\n",
        "\n",
        "        f2 : filter weights for second conv layer of shape \n",
        "        (n_filters_2, n_filters_1, filter_size**2).\n",
        "\n",
        "        b2 : biases for second conv layer of shape (n_filters_2,).\n",
        "\n",
        "        gamma2 : scale factors for the second batch norm layer of shape \n",
        "        (n_filters_2,).\n",
        "\n",
        "        beta2 : shift factors for second batch norm layer of shape\n",
        "        (n_filters_2,).\n",
        "\n",
        "        mu2_MA, var2_MA : floats. Moving averages of the mean and variance for \n",
        "        second batch norm layer.\n",
        "\n",
        "        w1 : weights for the fully connected layer of shape \n",
        "        (n_dense, n_filters_2 * 49).\n",
        "\n",
        "        b3 : biases for the fully connected layer, shape (n_dense,). \n",
        "\n",
        "        gamma3 : scale factors for the third batch norm layer of shape \n",
        "        (n_dense,).\n",
        "\n",
        "        beta3 : shift factors for third batch norm layer of shape\n",
        "        (n_dense,).\n",
        "\n",
        "        mu3_MA, var3_MA : floats. Moving averages of the mean and variance for \n",
        "        third batch norm layer.\n",
        "\n",
        "        w2 : weights for the output layer, shape (10, n_dense).\n",
        "\n",
        "        b4 : biases for output layer, shape (10,).\n",
        "        \"\"\"\n",
        "        img_size = batch.shape[1]\n",
        "        # first we pad the input with zeros\n",
        "        n_pad = int((self.filter_size - 1) / 2)\n",
        "        batch = np.pad(batch, ((0, 0), (n_pad, n_pad), (n_pad, n_pad)))\n",
        "        # first convolutional layer\n",
        "        arr1 = np.empty((self.filter_size ** 2, self.batch_size, img_size, img_size))\n",
        "        for i in range(self.filter_size ** 2):\n",
        "            arr1[i, :, :, :] = batch[:, i//self.filter_size:i//self.filter_size + img_size,\n",
        "                                        i%self.filter_size:i%self.filter_size + img_size]\n",
        "        c1 = np.tensordot(f1, arr1, axes=((1), (0))) + b1.reshape(self.n_filters_1, 1, 1, 1)\n",
        "        c1 = self.relu(c1)\n",
        "        # first max pooling layer\n",
        "        img_size_new = int(img_size / 2)\n",
        "        s1 = self.n_filters_1 * self.batch_size * img_size_new**2\n",
        "        z1 = np.arange(s1)\n",
        "        res1 = np.swapaxes(c1.reshape(self.n_filters_1, self.batch_size, img_size_new, 2,\n",
        "                                      img_size_new, 2), 3, 4).reshape(s1, 4)\n",
        "        arg1 = np.argmax(res1, axis=1)\n",
        "        mp1 = res1[z1, arg1].reshape(self.n_filters_1, self.batch_size, img_size_new, img_size_new)\n",
        "        # first batch normalisation\n",
        "        mu1 = np.mean(mp1, axis=(1, 2, 3))\n",
        "        m1 = self.batch_size * img_size_new**2\n",
        "        xmu1 = mp1 - mu1.reshape(self.n_filters_1, 1, 1, 1)\n",
        "        var1 = np.sum(xmu1 ** 2, axis=(1, 2, 3)) / m1\n",
        "        invsd1 = 1 / np.sqrt(var1.reshape(self.n_filters_1, 1, 1, 1) + 0.001)\n",
        "        x_hat1 = (mp1 - mu1.reshape(self.n_filters_1, 1, 1, 1)) * invsd1\n",
        "        bn1 = gamma1.reshape(self.n_filters_1, 1, 1, 1)*x_hat1 \\\n",
        "              + beta1.reshape(self.n_filters_1, 1, 1, 1)\n",
        "        # update moving averages for mean and var\n",
        "        mu1_MA = self.momentum*mu1_MA + (1 - self.momentum)*mu1\n",
        "        var1_MA = self.momentum*var1_MA + (1 - self.momentum)*var1\n",
        "        padded = np.pad(bn1, ((0, 0), (0, 0), (n_pad, n_pad), (n_pad, n_pad)))\n",
        "        # second convolutional layer\n",
        "        arr2 = np.empty((self.filter_size ** 2, self.n_filters_1, self.batch_size, img_size_new, img_size_new))\n",
        "        for i in range(self.filter_size ** 2):\n",
        "            arr2[i, :, :, :, :] = padded[:, :, i//self.filter_size:i//self.filter_size + img_size_new,\n",
        "                                               i%self.filter_size:i%self.filter_size + img_size_new]\n",
        "        c2 = np.tensordot(f2, arr2, axes=((1, 2), (1, 0))) + b2.reshape(self.n_filters_2, 1, 1, 1)\n",
        "        c2 = self.relu(c2)\n",
        "        # second max pooling layer\n",
        "        img_size_new_2 = int(img_size_new / 2)\n",
        "        s2 = self.n_filters_2 * self.batch_size * img_size_new_2**2\n",
        "        z2 = np.arange(s2)\n",
        "        res2 = np.swapaxes(c2.reshape(self.n_filters_2, self.batch_size, img_size_new_2, 2,\n",
        "                                      img_size_new_2, 2), 3, 4).reshape(s2, 4)\n",
        "        arg2 = np.argmax(res2, axis=1)\n",
        "        mp2 = res2[z2, arg2].reshape(self.n_filters_2, self.batch_size, img_size_new_2, img_size_new_2)\n",
        "        # second batch normalisation\n",
        "        m2 = img_size_new_2**2 * self.batch_size\n",
        "        mu2 = np.mean(mp2, axis=(1, 2, 3))\n",
        "        xmu2 = mp2 - mu2.reshape(self.n_filters_2, 1, 1, 1)\n",
        "        var2 = np.sum(xmu2 ** 2, axis=(1, 2, 3)) / m2\n",
        "        invsd2 = 1 / np.sqrt(var2.reshape(self.n_filters_2, 1, 1, 1) + 0.001)\n",
        "        x_hat2 = (mp2 - mu2.reshape(self.n_filters_2, 1, 1, 1)) * invsd2\n",
        "        bn2 = gamma2.reshape(self.n_filters_2, 1, 1, 1)*x_hat2 \\\n",
        "              + beta2.reshape(self.n_filters_2, 1, 1, 1)\n",
        "        # update moving averages for mean and var\n",
        "        mu2_MA = self.momentum*mu2_MA + (1 - self.momentum)*mu2\n",
        "        var2_MA = self.momentum*var2_MA + (1 - self.momentum)*var2\n",
        "        # flatten the output\n",
        "        flat = np.swapaxes(bn2, 0, 1).reshape(self.batch_size, -1)\n",
        "        # dense layer\n",
        "        d1 = np.matmul(w1, flat.T) + b3[:, np.newaxis]\n",
        "        a1 = self.relu(d1)\n",
        "        # third batch normalisation\n",
        "        mu3 = np.mean(a1, axis=1)\n",
        "        xmu3 = a1 - mu3[:, np.newaxis]\n",
        "        var3 = 1 / self.batch_size * np.sum(xmu3 ** 2, axis=1)\n",
        "        invsd3 = 1 / np.sqrt(var3.reshape(self.n_dense, 1) + 0.001)\n",
        "        x_hat3 = (a1 - mu3.reshape(self.n_dense, 1)) * invsd3\n",
        "        bn3 = gamma3.reshape(self.n_dense, 1)*x_hat3 + beta3.reshape(self.n_dense, 1)\n",
        "        # update moving averages\n",
        "        mu3_MA = self.momentum*mu3_MA + (1 - self.momentum)*mu3\n",
        "        var3_MA = self.momentum*var3_MA + (1 - self.momentum)*var3\n",
        "        a2 = self.softmax(np.matmul(w2, bn3) + b4[:, np.newaxis])\n",
        "\n",
        "        # gradient via backprop - categorical cross entropy loss\n",
        "        inter1 = a2 - truth\n",
        "        db4 = np.sum(inter1, axis=1)\n",
        "        dw2 = np.matmul(inter1, bn3.T)\n",
        "        inter2 = np.matmul(w2.T, inter1)\n",
        "        dbeta3 = np.sum(inter2, axis=1)\n",
        "        dgam3 = np.sum(inter2 * x_hat3, axis=1)\n",
        "        dxhat3 = inter2 * gamma3[:, np.newaxis]\n",
        "        dsig3 = np.sum(dxhat3 * xmu3, axis=1) * 0.5 * (invsd3.reshape(self.n_dense) ** 3)\n",
        "        dmu3 = np.sum(dxhat3 * -invsd3, axis=1) + dsig3*np.sum(-2 * xmu3, axis=1)/self.batch_size\n",
        "        inter3 = (dxhat3*invsd3 + dsig3[:, np.newaxis]*2*xmu3/self.batch_size\n",
        "                  + dmu3[:, np.newaxis]/self.batch_size) * np.sign(a1)\n",
        "        db3 = np.sum(inter3, axis=1)\n",
        "        dw1 = np.matmul(inter3, flat)\n",
        "        inter4 = np.swapaxes(np.matmul(w1.T, inter3).T.reshape(self.batch_size, self.n_filters_2,\n",
        "                                                               img_size_new_2, img_size_new_2), 0, 1)\n",
        "        dbeta2 = np.sum(inter4, axis=(1, 2, 3))\n",
        "        dgam2 = np.sum(inter4 * x_hat2, axis=(1, 2, 3))\n",
        "        dxhat2 = inter4 * gamma2.reshape(self.n_filters_2, 1, 1, 1)\n",
        "        dsig2 = np.sum(dxhat2 * xmu2, axis=(1, 2, 3)) * 0.5 * (invsd2.reshape(self.n_filters_2) ** 3)\n",
        "        dmu2 = np.sum(dxhat2 * -invsd2, axis=(1, 2, 3)) + dsig2*np.sum(-2 * xmu2, axis=(1, 2, 3))/m2\n",
        "        inter5 = dxhat2*invsd2 + dsig2.reshape(self.n_filters_2, 1, 1, 1)*2*xmu2/m2 \\\n",
        "                 + dmu2.reshape(self.n_filters_2, 1, 1, 1)/m2\n",
        "        mp_grad2 = np.zeros((s2, 4))\n",
        "        mp_grad2[z2, arg2] = inter5.ravel()\n",
        "        mp_grad2 = np.swapaxes(mp_grad2.reshape(self.n_filters_2, self.batch_size, img_size_new_2, \n",
        "         img_size_new_2, 2, 2), 3, 4).reshape(self.n_filters_2, self.batch_size, img_size_new, img_size_new)\n",
        "        inter6 = mp_grad2 * np.sign(c2)\n",
        "        db2 = np.sum(inter6, axis=(1, 2, 3))\n",
        "        df2 = np.swapaxes(np.tensordot(inter6, arr2, axes=((1, 2, 3), (2, 3, 4))), 1, 2)\n",
        "        f2_prime = np.rot90(f2.reshape(self.n_filters_2, self.n_filters_1, self.filter_size, self.filter_size), \n",
        "                            2, axes=(2, 3))\n",
        "        pad = np.pad(inter6, ((0, 0), (0, 0), (n_pad, n_pad), (n_pad, n_pad)))\n",
        "        sub = np.empty((self.filter_size ** 2, self.n_filters_2, self.batch_size, img_size_new, img_size_new))\n",
        "        for i in range(self.filter_size ** 2):\n",
        "            sub[i, :, :, :, :] = pad[:, :, i//self.filter_size:i//self.filter_size + img_size_new,\n",
        "                                           i%self.filter_size:i%self.filter_size + img_size_new]\n",
        "        inter7 = np.tensordot(f2_prime.reshape(self.n_filters_2, self.n_filters_1, self.filter_size ** 2), sub,\n",
        "                              axes=((0, 2), (1, 0)))\n",
        "        dbeta1 = np.sum(inter7, axis=(1, 2, 3))\n",
        "        dgam1 = np.sum(inter7 * x_hat1, axis=(1, 2, 3))\n",
        "        dxhat1 = inter7 * gamma1.reshape(self.n_filters_1, 1, 1, 1)\n",
        "        dsig1 = np.sum(dxhat1 * xmu1, axis=(1, 2, 3)) * 0.5 * (invsd1.reshape(self.n_filters_1) ** 3)\n",
        "        dmu1 = np.sum(dxhat1 * -invsd1, axis=(1, 2, 3)) + dsig1 * np.sum(-2 * xmu1, axis=(1, 2, 3)) / m1\n",
        "        inter8 = dxhat1*invsd1 + dsig1.reshape(self.n_filters_1, 1, 1, 1)*2*xmu1/m1 \\\n",
        "                 + dmu1.reshape(self.n_filters_1, 1, 1, 1)/m1\n",
        "        mp_grad1 = np.zeros((s1, 4))\n",
        "        mp_grad1[z1, arg1] = inter8.ravel()\n",
        "        mp_grad1 = np.swapaxes(mp_grad1.reshape(self.n_filters_1, self.batch_size, img_size_new, img_size_new, \n",
        "                               2, 2), 3, 4).reshape(self.n_filters_1, self.batch_size, img_size, img_size)\n",
        "        inter9 = mp_grad1 * np.sign(c1)\n",
        "        db1 = np.sum(inter9, axis=(1, 2, 3))\n",
        "        df1 = np.tensordot(inter9, arr1, axes=((1, 2, 3), (1, 2, 3)))\n",
        "        return [df1, db1, dgam1, dbeta1, df2, db2, dgam2, dbeta2, dw1, db3, dgam3, dbeta3, dw2,\n",
        "                db4], mu1_MA, var1_MA, mu2_MA, var2_MA, mu3_MA, var3_MA\n",
        "\n",
        "    def _sgd(self, batch, truth, p, mu1, var1, mu2, var2, mu3, var3, learning_rate):\n",
        "        \"\"\"Implements stochastic gradient descent and returns updated parameters.\n",
        "\n",
        "        Parameters\n",
        "        ---------- \n",
        "        batch : numpy array of shape (batch_size, 28, 28) containing batch\n",
        "        of training data.\n",
        "\n",
        "        truth : numpy array of shape (n_classes, batch_size) containing the one\n",
        "        hot encoded true classes of each batch observation. \n",
        "\n",
        "        p : list containing all network parameter values. \n",
        "\n",
        "        mu1,...,var3 : current estimates of the batch norm means and variances.\n",
        "\n",
        "        learning_rate : float. Controls the size of the optimisation step taken.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        p, mu1,...,var3 : the updated values of the parameters, means and variances.\n",
        "        \"\"\"\n",
        "        grad, mu1, var1, mu2, var2, mu3, var3 = self._cnn_grad(batch, truth, p[0], p[1], p[2], p[3], mu1, var1, \n",
        "                                                p[4], p[5], p[6], p[7], mu2, var2, p[8], p[9], p[10], p[11], \n",
        "                                                mu3, var3, p[12], p[13])\n",
        "        p = [a - learning_rate * b for a, b in zip(p, grad)]\n",
        "        return p, mu1, var1, mu2, var2, mu3, var3\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"This function implements Glorot uniform initialisation of the weights \n",
        "        of the network and performs one gradient descent step for each batch in \n",
        "        each epoch. The trained weights, means and variances are stored in the \n",
        "        attributes par, mu and var.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        X : numpy array of shape (n_training_obs, 28, 28) containing the \n",
        "        training images.\n",
        "\n",
        "        y : numpy array of shape (n_classes, n_training_obs) containing the one \n",
        "        hot encoded true class of each training observation.\"\"\"\n",
        "        p = [np.random.uniform(-np.sqrt(6 / (self.filter_size**2 + self.n_filters_1)),\n",
        "            np.sqrt(6 / (self.filter_size**2 + self.n_filters_1)), (self.n_filters_1, self.filter_size ** 2)),\n",
        "            np.zeros(self.n_filters_1), np.ones(self.n_filters_1), np.zeros(self.n_filters_1),\n",
        "            np.random.uniform(-np.sqrt(6 / (self.n_filters_1 * self.filter_size**2 + self.n_filters_2)),\n",
        "                               np.sqrt(6 / (self.n_filters_1 * self.filter_size**2 + self.n_filters_2)),\n",
        "                              (self.n_filters_2, self.n_filters_1, self.filter_size ** 2)),\n",
        "            np.zeros(self.n_filters_2), np.ones(self.n_filters_2), np.zeros(self.n_filters_2),\n",
        "            np.random.uniform(-np.sqrt(6 / (self.n_filters_2 * 49 + self.n_dense)),\n",
        "                               np.sqrt(6 / (self.n_filters_2*49 + self.n_dense)), \n",
        "                              (self.n_dense, self.n_filters_2*49)),\n",
        "            np.zeros(self.n_dense), np.ones(self.n_dense), np.zeros(self.n_dense),\n",
        "            np.random.uniform(-np.sqrt(6 / (self.n_dense + 10)), np.sqrt(6 / (self.n_dense + 10)), \n",
        "                              (10, self.n_dense)),\n",
        "            np.zeros(10)]\n",
        "        mu1, var1, mu2, var2, mu3, var3 = 0, 1, 0, 1, 0, 1\n",
        "        for i in range(self.epochs):\n",
        "            # in each epoch perform gradient descent on each mini batch\n",
        "            perm = np.random.permutation(X.shape[0])\n",
        "            X = X[perm, :, :]\n",
        "            y = y[:, perm]\n",
        "            for j in range(int(np.ceil(X.shape[0] / self.batch_size))):\n",
        "                batch = X[self.batch_size * j:self.batch_size * (j + 1), :, :]\n",
        "                truth = y[:, self.batch_size * j:self.batch_size * (j + 1)]\n",
        "                p, mu1, var1, mu2, var2, mu3, var3 = self._sgd(batch, truth, p, mu1, var1, mu2, var2,\n",
        "                                                               mu3, var3, self.learning_rate)\n",
        "        self.par, self.mu, self.var = p, [mu1, mu2, mu3], [var1, var2, var3]\n",
        "        return p, mu1, var1, mu2, var2, mu3, var3\n",
        "\n",
        "    def predict(self, test_X):\n",
        "        \"\"\"This returns the predicted classes of the test data given by the CNN.\n",
        "\n",
        "        Parameters \n",
        "        ----------\n",
        "        test_X : numpy array of shape (n_test_samples, 28, 28) containing\n",
        "        the test data.\n",
        "        \"\"\"\n",
        "        pred = np.empty(test_X.shape[0])\n",
        "        batch_size = 1000\n",
        "        img_size = test_X.shape[1]\n",
        "        img_size_new = int(img_size/2)\n",
        "        img_size_new_2 = int(img_size_new/2)\n",
        "        for j in range(int(test_X.shape[0]/batch_size)):\n",
        "            n_pad = int((self.filter_size - 1) / 2)\n",
        "            batch = np.pad(test_X[batch_size * j:batch_size * (j + 1)], ((0,0), (n_pad, n_pad), (n_pad, n_pad)))\n",
        "            arr1 = np.empty((self.filter_size ** 2, batch_size, img_size, img_size))\n",
        "            for i in range(self.filter_size ** 2):\n",
        "                arr1[i, :, :, :] = batch[:, i//self.filter_size:i//self.filter_size + img_size,\n",
        "                                            i%self.filter_size:i%self.filter_size + img_size]\n",
        "            c1 = np.tensordot(self.par[0], arr1, axes=((1), (0))) \\\n",
        "                 + self.par[1].reshape(self.n_filters_1, 1, 1, 1)\n",
        "            c1 = self.relu(c1)\n",
        "            mp1 = c1.reshape(self.n_filters_1, batch_size, img_size_new, 2, img_size_new, 2).max(axis=(3, 5))\n",
        "            x_hat1 = (mp1 - self.mu[0].reshape(self.n_filters_1, 1, 1, 1)) \\\n",
        "                     / np.sqrt(self.var[0].reshape(self.n_filters_1, 1, 1, 1))\n",
        "            bn1 = self.par[2].reshape(self.n_filters_1, 1, 1, 1)*x_hat1 \\\n",
        "                  + self.par[3].reshape(self.n_filters_1, 1, 1, 1)\n",
        "            padded = np.pad(bn1, ((0, 0), (0, 0), (n_pad, n_pad), (n_pad, n_pad)))\n",
        "            arr2 = np.empty((self.filter_size ** 2, self.n_filters_1, batch_size, img_size_new, img_size_new))\n",
        "            for i in range(self.filter_size ** 2):\n",
        "                arr2[i, :, :, :, :] = padded[:, :, i//self.filter_size:i//self.filter_size + img_size_new,\n",
        "                                                   i%self.filter_size:i%self.filter_size + img_size_new]\n",
        "            c2 = np.tensordot(self.par[4], arr2, axes=((1, 2), (1, 0))) \\\n",
        "                 + self.par[5].reshape(self.n_filters_2, 1, 1, 1)\n",
        "            c2 = self.relu(c2)\n",
        "            mp2 = c2.reshape(self.n_filters_2, batch_size, img_size_new_2, 2, img_size_new_2, 2).max(axis=(3, 5))\n",
        "            x_hat2 = (mp2 - self.mu[1].reshape(self.n_filters_2, 1, 1, 1)) \\\n",
        "                     / np.sqrt(self.var[1].reshape(self.n_filters_2, 1, 1, 1))\n",
        "            bn2 = self.par[6].reshape(self.n_filters_2, 1, 1, 1)*x_hat2 \\\n",
        "                  + self.par[7].reshape(self.n_filters_2, 1, 1, 1)\n",
        "            flat = np.swapaxes(bn2, 0, 1).reshape(batch_size, -1)\n",
        "            d1 = np.matmul(self.par[8], flat.T) + self.par[9][:, np.newaxis]\n",
        "            a1 = self.relu(d1)\n",
        "            x_hat3 = (a1 - self.mu[2].reshape(self.n_dense, 1)) / np.sqrt(self.var[2].reshape(self.n_dense, 1))\n",
        "            bn3 = self.par[10].reshape(self.n_dense, 1)*x_hat3 + self.par[11].reshape(self.n_dense, 1)\n",
        "            a2 = self.softmax(np.matmul(self.par[12], bn3) + self.par[13][:, np.newaxis])\n",
        "            pred[j * batch_size:batch_size * (j + 1)] = np.argmax(a2, axis=0)\n",
        "        return pred"
      ],
      "metadata": {
        "id": "8DoQdPxxkVjO"
      },
      "execution_count": 5,
      "outputs": []
    }
  ]
}