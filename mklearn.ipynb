{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mklearn.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOxR4Vy6JozELcO3kTKNUdG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mattbarrett98/mykit-learn/blob/update/mklearn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "PBKDNN7dnfEc"
      },
      "outputs": [],
      "source": [
        "# fast linear algebra, useful for every algorithm\n",
        "import numpy as np\n",
        "\n",
        "# for minimising logistic loss and fast computation of log sum exponential\n",
        "import scipy.optimize\n",
        "from scipy.special import logsumexp \n",
        "\n",
        "# to create abstract base class for all classifiers\n",
        "from abc import ABCMeta, abstractmethod\n",
        "\n",
        "# to speed up algorithm for support vector classifier\n",
        "%load_ext Cython\n",
        "\n",
        "# to parallelise random forest\n",
        "import multiprocessing as mp"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cython code for support vector classifier"
      ],
      "metadata": {
        "id": "X9kBhaowbARg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%cython\n",
        "cimport numpy as np\n",
        "import numpy as np\n",
        "cimport cython\n",
        "from scipy.linalg.cython_blas cimport ddot\n",
        "from libc.stdlib cimport rand, srand\n",
        "from libc.time cimport time\n",
        "srand(time(NULL))\n",
        "\n",
        "\"\"\"This is our implementation of the sequential minimal optimisation algorithm\n",
        "(SMO) which we will use for the support vector classifier (SVC). It is defined \n",
        "here (and not in the block below with the rest of the code) because we have \n",
        "written it in Cython for performance improvement.\n",
        "\"\"\"\n",
        "\n",
        "cdef int mi_rand_int(int upper, int i):\n",
        "    \"\"\"Gives us a random integer in the range [0, upper] not equal to i.\"\"\"\n",
        "    cdef int j = i \n",
        "    while(j == i):\n",
        "        j = rand() % (upper)\n",
        "    return j  \n",
        "\n",
        "cdef double mi_dot(np.ndarray[double, ndim=1] a, np.ndarray[double, ndim=1] b):\n",
        "    \"\"\"We use the blas function ddot, which returns the dot product of two \n",
        "    1D arrays containing double precision values. This function is slightly \n",
        "    faster than numpy's dot(). \n",
        "    \"\"\" \n",
        "    cdef int n = a.shape[0]\n",
        "    cdef int stride = 1\n",
        "    return ddot(&n, &a[0], &stride, &b[0], &stride)\n",
        "\n",
        "@cython.boundscheck(False)\n",
        "@cython.wraparound(False)\n",
        "cpdef SMO(np.ndarray[double, ndim=2] X, np.ndarray[double, ndim=1] y, double C, \n",
        "         double epsilon, double tol, int max_iter):\n",
        "        \"\"\"This is where SMO is carried out. The implementation and notation \n",
        "        used is consistent with http://cs229.stanford.edu/materials/smo.pdf . \n",
        "        As well as using blas' ddot instead of np.dot, we use a few other tricks\n",
        "        to speed up the code. \n",
        "        \n",
        "        - We statically type all the variables e.g. 'cdef int count' specifies \n",
        "        that the variable 'count' is an integer.\n",
        "\n",
        "        - We disable bounds checking (decorators above the function). So each\n",
        "        time we index an array there aren't any checks for out of bounds or\n",
        "        negative indices. \n",
        "\n",
        "        These changes result in a 35% increase in speed compared to my original\n",
        "        python implementation. \n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : array of type float64 and shape (n_training_obs, n_training_obs). In\n",
        "        our implementation this is the radial basis function (RBF) kernel \n",
        "        applied to the training data.\n",
        "\n",
        "        y : array of type float64 and shape (n_training_obs, ). Contains the\n",
        "        true class of each training observation.  \n",
        "\n",
        "        C : float64. Regularisation parameter. \n",
        "\n",
        "        epsilon : float64. If the norm of the difference in the alpha parameters\n",
        "        from one iteration to the next is less than epsilon, the algorithm\n",
        "        terminates.\n",
        "\n",
        "        tol : float64. Tolerance- controls how small the error needs to be to \n",
        "        consider the corresponding alpha parameter optimal. \n",
        "\n",
        "        max_iter : int. Maximum number of iterations to perform. \n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        alpha : array of type float64 and shape (n_training_obs, ). Contains the\n",
        "        values of the alpha parameters which minimise our SVC loss.   \n",
        "\n",
        "        b : float64. The value of b in our optimal solution.    \n",
        "        \"\"\"\n",
        "        cdef int count = 0\n",
        "        cdef Py_ssize_t i, j, n\n",
        "        cdef double L, H, b, b1, b2, alpha_j, alpha_i, E_i, E_j, eta\n",
        "        n = X.shape[0]\n",
        "        cdef np.ndarray[double, ndim=1] diff\n",
        "        cdef np.ndarray[double, ndim=1] alpha = np.zeros((n))\n",
        "        b = 0\n",
        "        while True:\n",
        "            count += 1\n",
        "            alpha_prev = np.copy(alpha)\n",
        "            for i in range(n):\n",
        "                alpha_y = alpha * y\n",
        "                E_i = mi_dot(X[i, :], alpha_y) + b - y[i]\n",
        "                # if alpha_i doesn't satisfy optimal conditions then we try to \n",
        "                # optimise it \n",
        "                if ((E_i*y[i]<-tol and alpha[i]<C) or \n",
        "                   (E_i*y[i]>tol and alpha[i]>0)):  \n",
        "                    \n",
        "                    j = mi_rand_int(n, i)\n",
        "                    eta = 2*X[i,j] - 2\n",
        "                    if eta == 0:\n",
        "                        continue\n",
        "                    alpha_j, alpha_i = alpha[j], alpha[i]\n",
        "                    if y[i] != y[j]:\n",
        "                        L = max(0, alpha_j - alpha_i)\n",
        "                        H = min(C, C - alpha_i + alpha_j)\n",
        "                    else:\n",
        "                        L = max(0, alpha_i + alpha_j - C)\n",
        "                        H = min(C, alpha_i + alpha_j)\n",
        "\n",
        "                    E_j = mi_dot(X[j,:], alpha_y) + b - y[j]\n",
        "                    alpha[j] = alpha_j - y[j]*(E_i - E_j)/eta\n",
        "                    alpha[j] = max(alpha[j], L)\n",
        "                    alpha[j] = min(alpha[j], H)\n",
        "                    alpha[i] = alpha_i + y[i]*y[j]*(alpha_j - alpha[j])\n",
        "\n",
        "                    b1 = b - E_i - y[i]*(alpha[i] - alpha_i) \\\n",
        "                                 - y[j]*(alpha[j] - alpha_j)*X[i,j]\n",
        "                    \n",
        "                    b2 = b - E_j - y[j]*(alpha[j] - alpha_j) \\\n",
        "                                 - y[i]*(alpha[i] - alpha_i)*X[i,j]\n",
        "                    if alpha[i]>0 and alpha[i]<C:\n",
        "                        b = b1\n",
        "                    elif alpha[j]>0 and alpha[j]<C:\n",
        "                        b = b2\n",
        "                    else: \n",
        "                        b = (b1 + b2)/2    \n",
        "\n",
        "            # Check convergence\n",
        "            diff = alpha - alpha_prev\n",
        "            if mi_dot(diff, diff) < epsilon:\n",
        "                break\n",
        "\n",
        "            if count >= max_iter:\n",
        "                return alpha, b\n",
        "        \n",
        "        return alpha, b"
      ],
      "metadata": {
        "id": "3Qo41MeddeaF"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Base classes"
      ],
      "metadata": {
        "id": "T7vfNtUYwMgx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BaseClassifier:\n",
        "    \"\"\"Base classifier from which all our classifiers will inherit since they\n",
        "    all have some shared functionality. They each have a classification accuracy\n",
        "    associated with them and need a method to calculate that accuracy. \n",
        "\n",
        "    We also have a magic method to allow us to compare our implementation to \n",
        "    sklearn's. We consider the implementations to be equivalent if their\n",
        "    classification accuracies are within 0.5% of each other. \n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.accuracy = None\n",
        "\n",
        "    def evaluate_predictions(self, predictions, true_classes):\n",
        "        n_correct_predictions = sum(predictions == true_classes)\n",
        "        n_predictions = predictions.shape[0]\n",
        "        self.accuracy = 100 * n_correct_predictions/n_predictions\n",
        "        return\n",
        "\n",
        "    def __eq__(self, sklearn_model):\n",
        "        diff = self.accuracy - sklearn_model.accuracy\n",
        "        diff = round(diff, 2)\n",
        "        if diff == 0:\n",
        "            return \"True, mklearn's accuracy is the same as sklearn's.\"\n",
        "        if diff > 0 and diff <= 0.5:\n",
        "            return \"True, mklearn's accuracy is just {}% higher than sklearn's.\".format(diff)\n",
        "        if diff > 0 and diff > 0.5:\n",
        "            return \"False, mklearn's accuracy is {}% better than sklearn's.\".format(diff)   \n",
        "        \n",
        "        if diff < 0 and diff >= -0.5:\n",
        "                return \"True, mklearn's accuracy is just {}% lower than sklearn's.\".format(-diff)  \n",
        "        if diff < 0 and diff < -0.5:\n",
        "                return \"False, mklearn's accuracy is {}% lower than sklearn's.\".format(-diff)\n",
        "\n",
        "\n",
        "class AbstractClassifier(metaclass=ABCMeta):\n",
        "    \"\"\"Abstract base class which ensures that all of our classifiers have\n",
        "    a fit method and a predict method. Not much purpose to this other than \n",
        "    learning about metaclasses. \n",
        "    \"\"\"\n",
        "    @abstractmethod\n",
        "    def fit():\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def predict():\n",
        "        pass   "
      ],
      "metadata": {
        "id": "3ojimYXmnTKe"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "H1boQyl5nd1q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Preprocessing:\n",
        "    \"\"\"Class containing all necessary methods to get our data into the \n",
        "    correct format.\n",
        "\n",
        "    - flatten_image: converts nxn images into 1D arrays of length n**2.\n",
        "\n",
        "    - normalise_data: normalises all pixel values into [0,1].\n",
        "\n",
        "    - one_hot_encode: converts class value to an array of zeros, with a 1 at\n",
        "    the index corresponding to the class value e.g. 2->(0,0,1,0,0,0,0,0,0,0).\n",
        "\n",
        "    - get_svc_output: converts the class value to either +1 or -1. For \n",
        "    example, for the SVC for class '0' the function replaces any 0 in \n",
        "    train_y with +1 and -1 for any other class.  \n",
        "    \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def flatten_image(train_data, test_data):\n",
        "        train_flattened = train_data.reshape(train_data.shape[0], -1)\n",
        "        test_flattened = test_data.reshape(test_data.shape[0], -1)\n",
        "        return train_flattened, test_flattened\n",
        "\n",
        "    @staticmethod\n",
        "    def normalise_data(train_data, test_data, max_value=255):\n",
        "        return train_data/max_value, test_data/max_value\n",
        "\n",
        "    @staticmethod\n",
        "    def one_hot_encode(classes):\n",
        "        one_hot_encoding = np.zeros((classes.shape[0], len(set(classes))))\n",
        "        for i in range(len(one_hot_encoding)):\n",
        "            one_hot_encoding[i, classes[i]] = 1\n",
        "        return one_hot_encoding   \n",
        "\n",
        "    @staticmethod\n",
        "    def get_svc_output(train_y):\n",
        "        train_y_svc = -1 * np.ones((len(set(train_y)), train_y.shape[0]))\n",
        "        for j in range(train_y_svc.shape[0]):\n",
        "            for i in range(train_y_svc.shape[1]):\n",
        "                if train_y[i] == j:\n",
        "                    train_y_svc[j, i] = 1\n",
        "        return train_y_svc     "
      ],
      "metadata": {
        "id": "XuyEIgyNncrT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# K nearest neighbours"
      ],
      "metadata": {
        "id": "E9IbjP8xnpKF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MiKNeighboursClassifier(BaseClassifier, AbstractClassifier):\n",
        "    \"\"\"Classifier implementing the k-nearest-neighbours algorithm. The\n",
        "    algorithm works by finding the k training observations which are\n",
        "    closest to a test point in euclidean distance. The predicted class\n",
        "    of this test point is then given by the most frequent occurring class\n",
        "    of these k training points.\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    k : int. The 'k' in k-nearest-neighbours: the number of neighbours to\n",
        "    use in the algorithm.\n",
        "\n",
        "    train_x : numpy array of shape (n_training_obs, n_features) to store\n",
        "    the training data.\n",
        "\n",
        "    train_y : numpy array of shape (n_training_obs, ) to store the true\n",
        "    class of each training observation.\n",
        "\n",
        "    n_test_obs : int. Number of test observations. \n",
        "\n",
        "    n_splits : int. The number of groups we want to break the test data\n",
        "    into.\n",
        "\n",
        "    split_size : int. The number of points in each split of the test data. \n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, k, n_splits):\n",
        "        self.k = k\n",
        "        self.train_x = None\n",
        "        self.train_y = None\n",
        "        self.n_test_obs = None\n",
        "        self.n_splits = n_splits\n",
        "        self.split_size = None\n",
        "\n",
        "    def fit(self, train_x, train_y):\n",
        "        \"\"\"In KNN all the calculations are done once we have the test data.\n",
        "        Hence all we need to do here is store the training data. \n",
        "        \"\"\"\n",
        "        self.train_x = train_x\n",
        "        self.train_y = train_y\n",
        "\n",
        "    def _find_euclidean_distances(self, test_x):\n",
        "        \"\"\"This is a generator which returns the euclidean distances between\n",
        "        the training points and the ith split of test points. The reason we\n",
        "        split the test points up is that calculating the euclidean distance\n",
        "        between every training and test point in one go would use too much\n",
        "        memory.\n",
        "\n",
        "        The squared euclidean distance between vectors x and y is given by\n",
        "        (x - y).T * (x - y) which we can rewrite as x.T*x - 2x.T*y + y.T*y.\n",
        "        This formula allows us to calculate the euclidean distance between\n",
        "        every pair of vectors in the training data and the test data split\n",
        "        efficiently. It also has the added bonus that we only need to\n",
        "        compute x.T*x once, where x is the training data.\n",
        "\n",
        "        Also note that we are working with the squared euclidean distances,\n",
        "        since finding the k smallest euclidean distances is equivalent to\n",
        "        finding the k smallest squared euclidean distances.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        test_x : numpy array of shape (n_test_samples, n_features) containing\n",
        "        the test data.\n",
        "        \"\"\"\n",
        "        train_x_squared_norms = (self.train_x * self.train_x).sum(axis=1)\n",
        "        for i in range(self.n_splits):\n",
        "            # get ith split of test points and find euc distances\n",
        "            lower_idx = self.split_size * i\n",
        "            upper_idx = self.split_size * (i + 1)\n",
        "            test_i = test_x[lower_idx: upper_idx]\n",
        "            euc_dists = -2 * np.matmul(self.train_x, test_i.T)\n",
        "            euc_dists += train_x_squared_norms[:, np.newaxis]\n",
        "            euc_dists += (test_i * test_i).sum(axis=1).T[np.newaxis, :]\n",
        "            yield euc_dists\n",
        "\n",
        "    def predict(self, test_x):\n",
        "        \"\"\"This returns the predicted classes of the test data given by knn.\n",
        "\n",
        "        Parameters \n",
        "        ----------\n",
        "        test_x : numpy array of shape (n_test_samples, n_features) containing\n",
        "        the test data.\n",
        "        \"\"\"\n",
        "        self.n_test_obs = test_x.shape[0]\n",
        "        self.split_size = int(self.n_test_obs / self.n_splits)\n",
        "        preds = np.empty(self.n_test_obs)\n",
        "        euc_dist_generator = self._find_euclidean_distances(test_x)\n",
        "        for i in range(self.n_splits):\n",
        "            euc_distances = next(euc_dist_generator)\n",
        "            k_neighbours = np.argsort(euc_distances, axis=0)[0:self.k, :]\n",
        "            k_classes = self.train_y[k_neighbours]\n",
        "            class_counts = [np.bincount(k_classes[:, i], minlength=10)\n",
        "                            for i in range(self.split_size)]\n",
        "            class_counts_arr = np.asarray(class_counts)\n",
        "            lower_idx = self.split_size * i\n",
        "            upper_idx = self.split_size * (i + 1)\n",
        "            preds[lower_idx:upper_idx] = np.argmax(class_counts_arr, axis=1)\n",
        "        return preds"
      ],
      "metadata": {
        "id": "7z_Zln9onowZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multinomial logistic regression"
      ],
      "metadata": {
        "id": "sbEOqvlSn4Vo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MiLogisticClassifier(BaseClassifier, AbstractClassifier):\n",
        "    \"\"\"Classifier implementing multinomial logistic regression.\n",
        "\n",
        "    The classifier assumes each of the training point classes is \n",
        "    categorically distibuted with the class probabilites given by the \n",
        "    multinomial logistic model. To estimate the parameters of the model, \n",
        "    we will use Bayesian statistics. We set a standard normal prior on each\n",
        "    parameter and use the training data to find the posterior distribution\n",
        "    (or at least a function proportional to it). We will take the maximiser \n",
        "    of the posterior as our parameter estimates, known as the maximum a \n",
        "    posteriori (MAP). \n",
        "\n",
        "    Note that finding the maximum of the posterior is equivalent to finding \n",
        "    the minimum of the negative of the log posterior. So for numerical\n",
        "    reasons we work with this function instead.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.n_features = None\n",
        "        self.n_classes = None\n",
        "        self.MAP = None\n",
        "\n",
        "    def _neg_log_posterior_grad(self, betas, train_x, train_y):\n",
        "        \"\"\"We compute the gradient of the negative log posterior in order to\n",
        "        speed up convergence of the minimisation. \n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        betas : a vector of parameters of length (n_features+1) * n_classes,\n",
        "        since there is 1 parameter for each feature plus an intercept term,\n",
        "        for each class. \n",
        "\n",
        "        train_x : numpy array of shape (n_training_obs, n_features)\n",
        "        containing the training data.\n",
        "\n",
        "        train_y : numpy array of shape (n_training_obs, n_classes) \n",
        "        containing the one hot encoded true class of each training \n",
        "        observation.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        neg_log_posterior : float. The value of the negative log posterior \n",
        "        distribution evaluated at betas.\n",
        "\n",
        "        gradients.ravel() : flattened vector containing the gradients of the\n",
        "        negative log posterior, with respect to each of the parameters. \n",
        "        \"\"\"\n",
        "\n",
        "        # calculate value of negative log posterior for betas\n",
        "        beta_matrix = betas.reshape(self.n_classes, self.n_features + 1)\n",
        "        weights = beta_matrix[:, :-1]\n",
        "        intercepts = beta_matrix[:, -1]\n",
        "        p = np.matmul(train_x, weights.T)\n",
        "        p += intercepts\n",
        "        p -= logsumexp(p, axis=1).reshape(p.shape[0], 1)\n",
        "        neg_log_likelihood = -np.sum(p * train_y)\n",
        "        neg_log_prior = 0.5 * np.sum(weights ** 2)\n",
        "        neg_log_posterior = neg_log_likelihood + neg_log_prior\n",
        "\n",
        "        # calculate gradients\n",
        "        gradients = np.zeros((self.n_classes, self.n_features + 1))\n",
        "        p = np.exp(p)\n",
        "        diff = p - train_y\n",
        "        gradients[:, 0:self.n_features] = np.matmul(diff.T, train_x)\n",
        "        gradients[:, 0:self.n_features] += weights\n",
        "        gradients[:, -1] = diff.sum(axis=0)\n",
        "        return neg_log_posterior, gradients.ravel()\n",
        "\n",
        "    def fit(self, train_x, train_y):\n",
        "        \"\"\"Here we minimise the negative log posterior (using L-BFGS-B) in \n",
        "        order to find our MAP estimates. It assigns these estimates to the \n",
        "        MAP attribute.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        train_x : numpy array of shape (n_training_obs, n_features)\n",
        "        containing the training data.\n",
        "\n",
        "        train_y : numpy array of shape (n_training_obs, n_classes) \n",
        "        containing the one hot encoded true class of each training \n",
        "        observation.\n",
        "        \"\"\"\n",
        "        self.n_features = train_x.shape[1]\n",
        "        self.n_classes = train_y.shape[1]\n",
        "        n_parameters = (self.n_features + 1) * self.n_classes\n",
        "        optim = scipy.optimize.minimize(self._neg_log_posterior_grad,\n",
        "                                        np.random.normal(size=n_parameters),\n",
        "                                        args=(train_x, train_y),\n",
        "                                        jac='TRUE',\n",
        "                                        method='L-BFGS-B',\n",
        "                                        options={'maxiter': 750}\n",
        "                                        )\n",
        "        self.MAP = optim.x.reshape(self.n_classes, self.n_features + 1)\n",
        "\n",
        "    def predict(self, test_x):\n",
        "        \"\"\"Using our MAP estimates we calculate the class probabilities for \n",
        "        our test data. We take the class with the largest probability to be\n",
        "        the predicted output. \n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        test_x : numpy array of shape (n_test_obs, n_features) containing \n",
        "        the test data.\n",
        "        \"\"\"\n",
        "        weights_map = self.MAP[:, :-1].T\n",
        "        intercepts_map = self.MAP[:, -1]\n",
        "        class_prob_pred = np.matmul(test_x, weights_map) + intercepts_map\n",
        "        predictions = np.argmax(class_prob_pred, axis=1)\n",
        "        return predictions "
      ],
      "metadata": {
        "id": "tWIzaTHsn2gM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Support vector classifier"
      ],
      "metadata": {
        "id": "fpDTAkeYoEJu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MiSupportVectorClassifier(BaseClassifier, AbstractClassifier):\n",
        "    \"\"\"Implements a support vector classifier. An SVC is used for binary \n",
        "    classification problems. It first transforms the training data using\n",
        "    some kernel function, in our case this is the Radial basis function \n",
        "    (RBF) kernel. Then in this transformed space, the SVC finds the \n",
        "    hyperplane which best separates the training points for the 2 classes. \n",
        "    This hyperplane can then be used to classify a new test point. \n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    C : float. Regularisation parameter. \n",
        "\n",
        "    epsilon : float. If the norm of the difference in the alpha parameters\n",
        "    from one iteration to the next is less than epsilon, the SMO algorithm\n",
        "    terminates.\n",
        "\n",
        "    tol : float. Tolerance- controls how small the error needs to be to \n",
        "    consider the corresponding alpha parameter optimal. \n",
        "\n",
        "    max_iter : int. Maximum number of iterations to perform. \n",
        "\n",
        "    gamma : float. The scale to use in the calculation of the RBF kernel.\n",
        "\n",
        "    alphas : This is where the optimal values of alpha from SMO are stored.\n",
        "\n",
        "    b : This is where the optimal values of b from SMO are stored. \n",
        "\n",
        "    train_y : To store the training classes since it is needed in the\n",
        "    predict method. \n",
        "\n",
        "    train_x : Stores the training data also required in predict method.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, C, epsilon, tol, max_iter):\n",
        "        self.C = C\n",
        "        self.epsilon = epsilon\n",
        "        self.tol = tol\n",
        "        self.max_iter = max_iter\n",
        "        self.gamma = None\n",
        "        self.alphas = None\n",
        "        self.b = None\n",
        "        self.train_y = None\n",
        "        self.train_x = None  \n",
        "\n",
        "    def _compute_rbf_kernel(self, X, Y):\n",
        "        \"\"\"Takes 2 arrays and calculates the RBF kernel between every pair\n",
        "        of vectors in each array. It calculates the squared Euclidean \n",
        "        distance between every pair of vectors between X and Y, which is an\n",
        "        intermediate value needed to find the RBF kernels.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X, Y : Both 2D numpy arrays of floats. They can have any shape, \n",
        "        however their first axis sizes must match: X.shape[1] == Y.shape[1].  \n",
        "        \"\"\"\n",
        "        euc_dists = -2 * np.matmul(X, Y.T)\n",
        "        euc_dists += (X * X).sum(axis=1)[:, np.newaxis]\n",
        "        euc_dists += (Y * Y).sum(axis=1).T[np.newaxis, :]\n",
        "        np.fill_diagonal(euc_dists, 0)\n",
        "        rbf_kernel = np.exp(-self.gamma * euc_dists)\n",
        "        return rbf_kernel\n",
        "\n",
        "    def fit(self, train_x, train_y):\n",
        "        \"\"\"Since SVCs perform binary classification and we have 10 classes \n",
        "        we must fit 10 different SVCs (one for each class).\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        train_x : numpy array of shape (n_training_obs, n_features) \n",
        "        and type float, contains the training data.\n",
        "\n",
        "        train_y : numpy array of shape (n_classes, n_training_obs) which\n",
        "        contains the true classes of the training data (for each of the\n",
        "        n_classes SVCs). \n",
        "        \"\"\"\n",
        "        self.train_y = train_y\n",
        "        self.train_x = train_x\n",
        "        n_classifiers = train_y.shape[0]\n",
        "        self.alphas = np.zeros((n_classifiers, train_x.shape[0]))\n",
        "        self.b = np.zeros(n_classifiers)\n",
        "        self.gamma = 1 / (train_x.shape[1] * train_x.var())\n",
        "        rbf_train_x = self._compute_rbf_kernel(train_x, train_x)\n",
        "        for i in range(n_classifiers):\n",
        "            self.alphas[i, :], self.b[i] = SMO(rbf_train_x,\n",
        "                                                train_y[i, :],\n",
        "                                                self.C,\n",
        "                                                self.epsilon,\n",
        "                                                self.tol,\n",
        "                                                self.max_iter)\n",
        "\n",
        "    def predict(self, test_x):\n",
        "        \"\"\"Calculates the predicted outputs of each of the binary SVCs. The\n",
        "        prediction for each test point is given by the highest (most\n",
        "        confident) output. This is known as one-vs-all classification. \n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        test_x : numpy array of shape (n_test_obs, n_features) and type\n",
        "        float. Contains the test points to make predictions on. \n",
        "        \"\"\"\n",
        "        rbf_test_x = self._compute_rbf_kernel(test_x, self.train_x)\n",
        "        svc_outputs = np.matmul(self.train_y * self.alphas, rbf_test_x.T) \\\n",
        "                      + self.b[:, np.newaxis]\n",
        "        predictions = np.argmax(svc_outputs, axis=0)\n",
        "        return predictions  "
      ],
      "metadata": {
        "id": "mLo0sYsEoCnJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decision tree"
      ],
      "metadata": {
        "id": "0tuBmvIrhzdQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MiDecisionTreeClassifier(BaseClassifier, AbstractClassifier):\n",
        "    \"\"\"Implements a classifier based on a decision tree. It works by asking \n",
        "    a series of questions about the data point. For example, is the value of\n",
        "    the 10th feature greater or less than 0.5. Based on the answer to this \n",
        "    it would then ask another question, and then another up to a maximum of \n",
        "    max_depth questions. The predicted class of a point is given by the most\n",
        "    frequent occurring class of the training points which followed the same \n",
        "    path down this 'tree' of questions. \n",
        "\n",
        "    In order to figure out what questions are best to ask, we use a measure\n",
        "    called the gini impurity. It measures how well a question splits up the\n",
        "    data, with a question that splits data perfectly having a gini impurity \n",
        "    of 0. We calculate the gini impurity of every possible way of splitting\n",
        "    the data, and choose the question with the lowest gini impurity.  \n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    max_depth : int, default=None (no restriction on tree depth). The\n",
        "    maximum number of questions we will ask about a point. \n",
        "\n",
        "    min_samples_split : int, default=2. The minimum number of training\n",
        "    points we need at any point in the tree in order to continue asking \n",
        "    questions. \n",
        "\n",
        "    min_gini : float, default=1e-5. If the gini impurity of a split is\n",
        "    lower than min_gini, then we stop asking questions i.e. we consider the\n",
        "    classes of the data to be pure enough to make good predictions. \n",
        "\n",
        "    tree : dict. This is where we store all our questions once we have\n",
        "    fitted the tree. \n",
        "    \"\"\"\n",
        "        \n",
        "    def __init__(self, max_depth=None, min_samples_split=2, min_gini=1e-5):\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.min_gini = min_gini\n",
        "        self.tree = None\n",
        "\n",
        "    def _find_best_split(self, train_x, train_y):\n",
        "        \"\"\"Function which finds the best possible way to split the data. It\n",
        "        calculates the gini impurity for every possible way of splitting\n",
        "        train_x i.e. for every feature and every value of each feature which\n",
        "        uniquely splits train_x. \n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        train_x : numpy array of shape (n_training_obs, n_features) and type\n",
        "        float. Contains the training data. \n",
        "\n",
        "        train_y : numpy array of shape (n_training_obs, n_classes) which \n",
        "        contains the one hot encoded true classes of the training data. \n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        best_split_gini : float. The value of the gini impurity for the best\n",
        "        split of the data. \n",
        "\n",
        "        split_feature : int. The feature that is used in the best split. \n",
        "\n",
        "        split_value : float. The value of split_feature that is used in the \n",
        "        best split e.g. the best split is obtained by asking the question\n",
        "        is split_feature less than or equal to split_value. \n",
        "        \"\"\"\n",
        "        sorted_x = np.argsort(train_x, axis=0)\n",
        "        n_features = train_x.shape[1]\n",
        "        n_classes = train_y.shape[1]\n",
        "        n_obs = train_x.shape[0]\n",
        "        # count the number of appearances of each class that appear above\n",
        "        # or below each split (we have n_obs-1 possible splits per feature)\n",
        "        class_counts_below = np.empty((n_obs - 1, n_features, n_classes))\n",
        "        class_counts_above = np.empty((n_obs - 1, n_features, n_classes))\n",
        "        n_obs_below = np.arange(1, n_obs)[:, None]\n",
        "        n_obs_above = np.arange(1, n_obs)[::-1][:, None]\n",
        "        # class counts after the first split for each feature\n",
        "        class_counts_below[0] = train_y[sorted_x[0, :]]\n",
        "        class_counts_above[0] = np.sum(train_y[sorted_x[1:, ]], axis=0)\n",
        "        # class counts for all subsequent possible splits\n",
        "        for i in range(1, n_obs - 1):\n",
        "            class_counts_below[i] = class_counts_below[i - 1] \\\n",
        "                                    + train_y[sorted_x[i]]\n",
        "            class_counts_above[i] = class_counts_above[i - 1] \\\n",
        "                                    - train_y[sorted_x[i]]\n",
        "        probabilities_below = class_counts_below / n_obs_below[:, :, None]\n",
        "        probabilities_above = class_counts_above / n_obs_above[:, :, None]\n",
        "        ginis_below = 1 - np.sum(probabilities_below ** 2, axis=2)\n",
        "        ginis_above = 1 - np.sum(probabilities_above ** 2, axis=2)\n",
        "        gini_impurities = (ginis_below*n_obs_below\n",
        "                          + ginis_above*n_obs_above) / n_obs\n",
        "        # smallest gini impurity is the best\n",
        "        arg_best = np.argmin(gini_impurities)\n",
        "        unravelled_args = np.unravel_index(arg_best, gini_impurities.shape)\n",
        "        best_split_gini = gini_impurities[unravelled_args]\n",
        "        split_feature = unravelled_args[1]\n",
        "        split_idx = unravelled_args[0]\n",
        "        feature_arg_sort = sorted_x[:, split_feature]\n",
        "        feature_values = train_x[feature_arg_sort, split_feature]\n",
        "        split_value = (feature_values[split_idx]\n",
        "                        + feature_values[split_idx + 1]) / 2\n",
        "        return best_split_gini, split_feature, split_value\n",
        "\n",
        "    def _perform_split(self, train_x, train_y, feature, split_value):\n",
        "        \"\"\"Once we have found the best possible split, this function will\n",
        "        actually split the training data up for us. \n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        train_x : numpy array of shape (n_training_obs, n_features) and type\n",
        "        float. Contains the training data. \n",
        "\n",
        "        train_y : numpy array of shape (n_training_obs, n_classes) which \n",
        "        contains the one hot encoded true classes of the training data. \n",
        "\n",
        "        feature : int. Feature used to perform split.\n",
        "\n",
        "        split_value : float. Value of the feature used to perform split. \n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        train_x_below, train_y_below : the points in train_x and train_y\n",
        "        whose value of 'feature' is less than or equal to split_value.\n",
        "\n",
        "        train_x_above, train_y_above : the points in train_x and train_y\n",
        "        whose value of 'feature' is greater than split_value. \n",
        "        \"\"\"\n",
        "        idx = train_x[:, feature] <= split_value\n",
        "        train_x_below = train_x[idx]\n",
        "        train_x_above = train_x[~idx]\n",
        "        train_y_below = train_y[idx]\n",
        "        train_y_above = train_y[~idx]\n",
        "        return train_x_below, train_x_above, train_y_below, train_y_above\n",
        "\n",
        "    def _build_tree(self, X, y, count):\n",
        "        \"\"\"A function to recursively build our tree. As long as max_depth\n",
        "        hasn't been reached, we have at least min_samples_split and the \n",
        "        impurity is greater than min_gini, then we continue splitting the\n",
        "        data by calling this function again. If one of these conditions is\n",
        "        violated we stop splitting the data and note the most frequently\n",
        "        occurring class of the remaining data. \n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : 2D numpy array of type float and shape (n, n_features) where n\n",
        "        is the number of training data points remaining at the current node.\n",
        "\n",
        "        y : numpy array of shape (n, n_classes) which contains the one hot \n",
        "        encoded true classes of the remaining training data. \n",
        "\n",
        "        count : int. Keeps track of the tree depth. \n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        sub_tree : dict. Contains the questions that we have chosen and the \n",
        "        most frequent occurring class found at the end of any path through \n",
        "        the tree.\n",
        "        \"\"\"\n",
        "        if self.max_depth is None:\n",
        "            depth_cond = True\n",
        "        else:\n",
        "            if count < self.max_depth:\n",
        "                depth_cond = True\n",
        "            else:\n",
        "                depth_cond = False\n",
        "        if X.shape[0] >= self.min_samples_split:\n",
        "            samples_cond = True\n",
        "        else:\n",
        "            samples_cond = False\n",
        "\n",
        "        if depth_cond & samples_cond:\n",
        "            gini, feature, split_value = self._find_best_split(X, y)\n",
        "            if gini >= self.min_gini:\n",
        "                count += 1\n",
        "                x_below, x_above, y_below, y_above = self._perform_split(\n",
        "                                                              X, \n",
        "                                                              y, \n",
        "                                                              feature, \n",
        "                                                              split_value)\n",
        "\n",
        "                question = \"{} <= {}\".format(feature, split_value)\n",
        "                sub_tree = {question: []}\n",
        "                # we use recursion to build the tree\n",
        "                next_node_below = self._build_tree(x_below, y_below, count)\n",
        "                next_node_above = self._build_tree(x_above, y_above, count)\n",
        "                sub_tree[question].append(next_node_below)\n",
        "                sub_tree[question].append(next_node_above)\n",
        "            else:\n",
        "                return np.argmax(np.sum(y, axis=0))\n",
        "        else:\n",
        "            return np.argmax(np.sum(y, axis=0))\n",
        "        return sub_tree \n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"Initialises tree depth to 0 and builds the tree using training\n",
        "        data X and their one hot encoded true classes y. \n",
        "        \"\"\"\n",
        "        count = 0\n",
        "        self.tree = self._build_tree(X, y, count) \n",
        "        return self.tree         \n",
        "\n",
        "    def make_prediction(self, test_obs, tree):\n",
        "        \"\"\"Recursively answers questions in our dict until it reaches the\n",
        "        end of the tree and outputs the prediction for that path. \n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        test_obs : numpy array of shape (n_features, ) and type float. The \n",
        "        test point that we want to make a prediction for. \n",
        "\n",
        "        tree : dict. Here we pass the fitted tree found in fit(). \n",
        "        \"\"\"\n",
        "        question = list(tree.keys())[0]\n",
        "        if test_obs[int(question.split()[0])] <= float(question.split()[2]):\n",
        "            answer = tree[question][0]\n",
        "        else:\n",
        "            answer = tree[question][1]\n",
        "        if not isinstance(answer, dict):\n",
        "            return answer\n",
        "        return self.make_prediction(test_obs, answer)\n",
        "\n",
        "    def predict(self, test_x):\n",
        "        \"\"\"Makes prediction for each test data point. \n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        test_x : numpy array of shape (n_test_obs, n_features) and type\n",
        "        float. \n",
        "        \"\"\"\n",
        "        n_test_obs = test_x.shape[0]\n",
        "        predictions = np.empty(n_test_obs)\n",
        "        for i in range(n_test_obs):\n",
        "            predictions[i] = self.make_prediction(test_x[i, :], self.tree)\n",
        "        return predictions   "
      ],
      "metadata": {
        "id": "W0bQ-ic1hErM"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random forest"
      ],
      "metadata": {
        "id": "9c5BQFMPoTcS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MiRandomForestClassifier(BaseClassifier, AbstractClassifier):\n",
        "    \"\"\"Implements a classifier based on the random forest algorithm. This\n",
        "    algorithm is just an ensemble of decision trees, which is where the\n",
        "    forest part comes from.The random part comes from how we fit each of\n",
        "    the decision trees. For each tree we resample the training points with\n",
        "    replacement to form a different set of training points (of the same size\n",
        "    as the original). We also take a random subset of features with which\n",
        "    we fit the tree. This randomness is what will allow our classifier to\n",
        "    generalise better than a single decision tree, which is inherently prone\n",
        "    to overfitting.  \n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    n_trees : int, default=1. The number of decision trees to fit. \n",
        "\n",
        "    max_features : int, default=100. The number of features to use in each\n",
        "    decision tree. \n",
        "\n",
        "    base_estimator : instance of DecisionTreeClassifier with parameters\n",
        "    max_depth, min_samples_split and min_gini. \n",
        "\n",
        "    n_jobs : int, default=1. The number of parallel processes to run. Should \n",
        "    be set to the number of cpu cores on your machine.\n",
        "\n",
        "    forests : this is where all of the fitted decision trees (and what \n",
        "    subset of features they used) is stored.  \n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_trees=100, max_features=100, max_depth=20,\n",
        "                  min_samples_split=2, min_gini=1e-5, n_jobs=1):\n",
        "        self.n_trees = n_trees\n",
        "        self.max_features = max_features\n",
        "        self.base_estimator = MiDecisionTreeClassifier(max_depth, \n",
        "                                                      min_samples_split, \n",
        "                                                      min_gini)\n",
        "        self.n_jobs = n_jobs\n",
        "        self.forests = None\n",
        "\n",
        "    def _build_forest(self, X, y, n_trees):\n",
        "        \"\"\"Function to fit a number of decision trees to form a forest.\n",
        "        Remember for each tree we use a different random selection of\n",
        "        training points and features. \n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : 2D numpy array of shape (n_training_obs, n_features) and type\n",
        "        float. Contains the training data.\n",
        "\n",
        "        y : numpy array of shape (n_training_obs, n_classes) which contains\n",
        "        the one hot encoded true classes of the training data. \n",
        "\n",
        "        n_trees : int. The number of trees to fit in this forest. \n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        trees : list of length n_trees containing dicts. Each dict being one\n",
        "        of the fitted trees. \n",
        "\n",
        "        feature_subsets : list of length n_trees, containing numpy arrays of\n",
        "        shape (max_features, ). Each array contains the subset of features\n",
        "        that each of the trees has been fit on. \n",
        "        \"\"\"\n",
        "        trees = []\n",
        "        feature_subsets = []\n",
        "        for i in range(n_trees):\n",
        "            new_features = np.random.choice(X.shape[1], self.max_features, \n",
        "                                            replace=False)\n",
        "            new_obs = np.random.choice(X.shape[0], X.shape[0])\n",
        "            new_X = X[new_obs, :][:, new_features]\n",
        "            new_y = y[new_obs]\n",
        "            trees.append(self.base_estimator.fit(new_X, new_y))\n",
        "            feature_subsets.append(new_features)\n",
        "        return trees, feature_subsets\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"We use the multiprocessing module to construct our forest in \n",
        "        parallel. For example, if we have n_jobs=2 and n_trees=100, then this\n",
        "        function will construct 2 forests each of 50 trees and construct them\n",
        "        in parallel. Note the use of a context manager in the 'with' statement.\n",
        "        This ensures the pool of processes is closed regardless of whether the\n",
        "        code inside errors. \n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : 2D numpy array of shape (n_training_obs, n_features) and type\n",
        "        float. Contains the training data.\n",
        "\n",
        "        y : numpy array of shape (n_training_obs, n_classes) which contains\n",
        "        the one hot encoded true classes of the training data. \n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        self.forests : contains a list of the fitted trees and feature \n",
        "        subsets from each parallel process. \n",
        "        \"\"\"\n",
        "        n_trees_job = int(self.n_trees / self.n_jobs)\n",
        "        with mp.Pool(processes=self.n_jobs) as pool:\n",
        "            self.forests = pool.starmap(self._build_forest, \n",
        "                                        [(X, y, n_trees_job) \n",
        "                                        for _ in range(self.n_jobs)])\n",
        "        return self.forests\n",
        "\n",
        "    def _tree_prediction(self, test_x, tree):\n",
        "        \"\"\"Make prediction of every test observation for a single tree.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        test_x : numpy array of shape (n_test_obs, n_features) and type\n",
        "        float.\n",
        "\n",
        "        tree : dict containing the fitted tree. \n",
        "        \"\"\"\n",
        "        n_test_obs = test_x.shape[0]\n",
        "        predictions = np.empty(n_test_obs)\n",
        "        for i in range(n_test_obs):\n",
        "            predictions[i] = self.base_estimator.make_prediction(test_x[i], \n",
        "                                                                  tree)\n",
        "        return predictions\n",
        "\n",
        "    def _forest_prediction(self, test_x, trees, feature_subsets):\n",
        "        \"\"\"Gives the prediction of every test observation, for each tree in\n",
        "        a forest. \n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        test_x : numpy array of shape (n_test_obs, n_features) and type\n",
        "        float.\n",
        "\n",
        "        trees : list of dicts containing the fitted trees in the forest.\n",
        "\n",
        "        feature_subsets : list of numpy arrays. Each array contains the subset\n",
        "        of features used in each of the trees. \n",
        "        \"\"\"\n",
        "        n_trees_job = int(self.n_trees / self.n_jobs)\n",
        "        predictions = np.empty((n_trees_job, test_x.shape[0]))\n",
        "        for i in range(n_trees_job):\n",
        "            predictions[i] = self._tree_prediction(\n",
        "                                    test_x[:, feature_subsets[i]], trees[i])\n",
        "        return predictions\n",
        "\n",
        "    def predict(self, test_x):\n",
        "        \"\"\"Function returns the predictions of the random forest. Again we use \n",
        "        multiprocessing to split the task into chunks for a performance boost. \n",
        "        From fit() method we constructed n_jobs number of sub-forests. Here we \n",
        "        make the predictions for each of those sub-forests in parallel. \n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        test_x : numpy array of shape (n_test_obs, n_features) and type\n",
        "        float.\n",
        "        \"\"\"\n",
        "        with mp.Pool(processes=self.n_jobs) as pool:\n",
        "            parallel_predictions = pool.starmap(self._forest_prediction, \n",
        "                                                [(test_x, \n",
        "                                                self.forests[i][0], \n",
        "                                                self.forests[i][1])\n",
        "                                                for i in range(self.n_jobs)])\n",
        "        all_predictions = np.vstack(parallel_predictions)\n",
        "        final_predictions = np.empty(test_x.shape[0])\n",
        "        for i in range(test_x.shape[0]):\n",
        "            counts = np.bincount(all_predictions[:, i].astype(int), minlength=10)\n",
        "            final_predictions[i] = np.argmax(counts)\n",
        "        return final_predictions            "
      ],
      "metadata": {
        "id": "U0OQsB7ZsxPo"
      },
      "execution_count": 5,
      "outputs": []
    }
  ]
}